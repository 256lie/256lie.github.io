This post is a living document of arguments and counterarguments of safety risks concerning long-term, general-purpose artificial intelligence (GAI). 



**Pathways towards GAI**

* Scaling existing methods with more compute 
* Whole brain simulation
* Theoretical breakthroughs in causality, reasoning, understanding, planning



**Likely existential risks if GAI is feasible**

* Goal mis-alignment 
  * Treacherous turn
  * Instrumental subgoals
  * Underspecified optimization (doing what is said instead of what was meant)
* Fast takeoff & recursive improvement
* AI cooperation and competition (international, commercial, government)



**Concerns of overemphasis of risks posed by GAI**

* Not much formal research or presentation amounting to lack of engagement with mainstream ML/AI communities
* Anthropomorphic projection onto "intelligence", "values", "agent"
* Much of technical safety research assumes RL will play a major component of GAI
* Rapid takeoff assumes almost unbounded intelligence curve that does not taper and without physical constraints 
  * the world is so stochastic e.g. weather prediction, molecular simulations stock markets, etc.
  * super intelligence capabilities at the level of academia, large corporations, or nation-states may be evolutionary optimal
  * algorithms of computational intelligence do not get trapped in sink holes like the Halting problem or hit hardware limitations (requiring fusion reactors and quantum computers)
* Alignment is not concomitant with a deadline of fast takeoff and there will be a gradual ramp towards GAI that allows for controls and quality systems, institutions, and policies to be in place and test cases that manifest safety issues 
  * alignment and safety is entangled and iteratively carried forward/scaled
  * 



2020 - 52

2019 - 24

2018 - 19

2017 - 5

2016 - 4

2015 - 1

2014 - 2

2013 - 0

2012 - 2

2011 - 1